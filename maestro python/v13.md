


# Maestro V13 — Full Implementation Plan

## File Map

```
Maestro/
├── .env                          # API keys (GEMINI_API_KEY, ANTHROPIC_API_KEY, OPENAI_API_KEY)
│
├── # === ENGINE (one per model provider) ===
├── maestro_v13_gemini.py         # Chat loop — Gemini 3 Pro
├── maestro_v13_opus.py           # Chat loop — Claude Opus 4.6
├── maestro_v13_gpt.py            # Chat loop — GPT-5.2
│
├── # === IDENTITY ===
├── experience_v13.py             # Who Maestro is, how he behaves
├── learning.py                   # Modifies experience from feedback (unchanged from V10)
│
├── # === KNOWLEDGE ===
├── knowledge_v13.py              # Loads knowledge_store/ into memory at startup
├── ingest.py                     # CLI: PDF → PNG → Pass 1 → Pass 2 → knowledge_store/
├── gemini_service.py             # Pass 1 + Pass 2 Gemini calls (captures all output)
│
├── # === TOOLS ===
├── tools_v13.py                  # All tool declarations + functions
├── vision.py                     # see_page, see_pointer, find_missing_pointer, double_check_pointer
│
├── # === PROMPTS ===
├── prompts/
│   ├── pass1.txt                 # Pass 1: Full page comprehension
│   └── pass2.txt                 # Pass 2: Pointer deep read
│
├── # === GENERATED DATA ===
├── knowledge_store/
│   └── {project_name}/
│       ├── project.json
│       ├── index.json
│       └── pages/
│           └── {page_name}/
│               ├── page.png
│               ├── pass1.json
│               └── pointers/
│                   └── {region_id}/
│                       ├── crop.png
│                       ├── pass2.json
│                       └── trace_*.png
│
├── # === HISTORY (untouched) ===
├── maestro.py                    # V1-V6
├── maestro_v7.py
├── maestro_v8.py
├── maestro_v9.py
├── maestro_v10.py
├── maestro_v11.py
├── maestro_v12_gemini.py
├── maestro_v12_opus.py
├── maestro_v12_gpt.py
├── experience.py
├── experience_v10.py
├── knowledge.py
├── tools.py
└── README.md
```

---

## File-by-File Breakdown

---

### `ingest.py` — The Knowledge Builder

**Run:** `python ingest.py "D:\Plans\CFA Love Field"`

**What it does:**
1. Scans folder for PDFs
2. Renders each page to PNG (PyMuPDF, 200 DPI)
3. Runs Pass 1 on each page (full page → Gemini)
4. Crops regions using Gemini's own `part.as_image()` outputs (NOT PIL re-crops from JSON)
5. Falls back to PIL crop from bbox only if Gemini didn't produce an image for that region
6. Runs Pass 2 on each cropped region
7. Builds aggregated `index.json`
8. Prints progress throughout

```python
# ingest.py

import sys, os, json, time
from pathlib import Path
import fitz  # PyMuPDF
from gemini_service import run_pass1, run_pass2, _save_trace

def render_page(pdf_path, page_num, output_path, dpi=200):
    """Render a PDF page to PNG. Returns (width, height)."""
    doc = fitz.open(pdf_path)
    page = doc[page_num]
    zoom = dpi / 72
    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))
    pix.save(str(output_path))
    doc.close()
    return pix.width, pix.height

def discover_pdfs(folder):
    """Scan folder for PDFs. Returns list of {path, name, page_count, discipline}."""
    # Infer discipline from folder name (Architectural, MEP, etc.)
    # Return list of dicts
    ...

def generate_region_id(bbox):
    """Deterministic ID from bbox position: 'r_{x0}_{y0}_{x1}_{y1}'"""
    return f"r_{bbox['x0']}_{bbox['y0']}_{bbox['x1']}_{bbox['y1']}"

def crop_region_pil(image_path, bbox, output_path, padding=20):
    """Fallback: PIL crop using 0-1000 normalized bbox."""
    from PIL import Image
    img = Image.open(image_path)
    w, h = img.size
    x0 = max(0, int(bbox["x0"] / 1000 * w) - padding)
    y0 = max(0, int(bbox["y0"] / 1000 * h) - padding)
    x1 = min(w, int(bbox["x1"] / 1000 * w) + padding)
    y1 = min(h, int(bbox["y1"] / 1000 * h) + padding)
    img.crop((x0, y0, x1, y1)).save(str(output_path))

def build_index(project_dir):
    """Aggregate all pass1/pass2 data into index.json."""
    # Walk all pages, collect:
    #   materials → {material: [sources]}
    #   keywords → {keyword: [sources]}
    #   modifications → [{action, item, source}]
    #   cross_refs → {sheet: [referenced_by]}
    #   broken_refs → [refs that point to pages not in the set]
    ...

def ingest(folder_path):
    project_name = Path(folder_path).stem
    store = Path("knowledge_store") / project_name
    
    pdfs = discover_pdfs(folder_path)
    total_pages = sum(p["page_count"] for p in pdfs)
    page_num = 0
    
    for pdf in pdfs:
        for pg in range(pdf["page_count"]):
            page_num += 1
            page_name = get_page_name(pdf["path"], pg)
            page_dir = store / "pages" / page_name
            page_dir.mkdir(parents=True, exist_ok=True)
            
            # Step 1: Render PNG
            print(f"[{page_num}/{total_pages}] Rendering {page_name}...", end=" ", flush=True)
            w, h = render_page(pdf["path"], pg, page_dir / "page.png")
            print(f"done ({w}x{h})")
            
            # Step 2: Pass 1
            print(f"[{page_num}/{total_pages}] Pass 1 {page_name}...", end=" ", flush=True)
            t = time.time()
            pass1 = run_pass1(page_dir / "page.png", page_name, pdf["discipline"])
            elapsed = time.time() - t
            
            regions = pass1.get("regions", [])
            crop_candidates = pass1.pop("_crop_candidates", [])  # raw bytes — remove before JSON
            
            # Save pass1 trace images to disk and update trace paths
            _save_trace(pass1.get("_trace", []), crop_candidates, page_dir, prefix="pass1_img")
            
            # Save pass1.json (no binary data — _crop_candidates already popped)
            with open(page_dir / "pass1.json", "w") as f:
                json.dump(pass1, f, indent=2)
            
            print(f"{len(regions)} regions, {len(crop_candidates)} images ({elapsed:.1f}s)")
            
            # Step 3: Crop regions + Pass 2
            for idx, region in enumerate(regions):
                region_id = generate_region_id(region["bbox"])
                pointer_dir = page_dir / "pointers" / region_id
                pointer_dir.mkdir(parents=True, exist_ok=True)
                
                # Crop source: PIL from bbox (reliable, deterministic)
                # Gemini's crop candidates are saved as trace images for reference,
                # but we don't try to match them to regions by order — too fragile.
                crop_region_pil(page_dir / "page.png", region["bbox"], pointer_dir / "crop.png")
                
                # Pass 2
                label = region.get("label", region_id)
                print(f"  Pass 2 [{idx+1}/{len(regions)}] {label}...", end=" ", flush=True)
                t = time.time()
                pass2 = run_pass2(
                    crop_path=pointer_dir / "crop.png",
                    page_path=page_dir / "page.png",
                    region=region,
                    pass1_context=pass1
                )
                elapsed = time.time() - t
                
                # Save pass2 trace images to disk and update trace paths
                pass2_images = pass2.pop("_trace_images", [])
                _save_trace(pass2.get("_trace", []), pass2_images, pointer_dir, prefix="trace_p2")
                
                # Save pass2.json (no binary data)
                with open(pointer_dir / "pass2.json", "w") as f:
                    json.dump(pass2, f, indent=2)
                
                print(f"done ({elapsed:.1f}s)")
    
    # Step 4: Build aggregated index
    print("Building index...", end=" ", flush=True)
    build_index(store)
    print("done")
    
    # Save project.json
    project_meta = {
        "name": project_name,
        "source_path": str(folder_path),
        "total_pages": total_pages,
        "disciplines": list(set(p["discipline"] for p in pdfs)),
        "ingested_at": time.strftime("%Y-%m-%dT%H:%M:%S")
    }
    with open(store / "project.json", "w") as f:
        json.dump(project_meta, f, indent=2)
    
    print(f"\nIngestion complete: {total_pages} pages → {store}")

if __name__ == "__main__":
    ingest(sys.argv[1])
```

---

### `gemini_service.py` — Pass 1 + Pass 2 (Captures Everything)

**Key changes from brain-mode-tuner:**
- NO `response_mime_type="application/json"` — let Gemini output naturally
- ALL parts captured into `_trace`
- Gemini's `part.as_image()` crops stored as region crop source
- Parse structured data from text output (not forced JSON)

```python
# gemini_service.py

import json, re, time, logging, io
from pathlib import Path
from google import genai
from google.genai import types
from dotenv import load_dotenv
import os

load_dotenv()
logger = logging.getLogger(__name__)

BRAIN_MODE_MODEL = "gemini-3-flash-preview"

def _get_client():
    return genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

def _extract_json_from_text(text):
    """Parse JSON from natural text output. Tries code blocks, then brace matching."""
    if not text:
        return {}
    
    # Try code block first
    match = re.search(r"```(?:json)?\s*\n?(.*?)\n?```", text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1).strip())
        except json.JSONDecodeError:
            pass
    
    # Try outermost braces
    start = text.find("{")
    end = text.rfind("}") + 1
    if start != -1 and end > start:
        candidate = text[start:end]
        # Fix trailing commas
        candidate = re.sub(r',\s*}', '}', candidate)
        candidate = re.sub(r',\s*]', ']', candidate)
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass
    
    return {}

def _collect_response(response):
    """Collect ALL parts from Gemini response into structured output.
    
    Captures every part type from the code execution API:
    - thought: model's internal reasoning (thinking mode)
    - text: natural language output
    - code: Python code the model wrote and executed
    - code_result: stdout/output from that code (+ outcome: OK or FAILED)
    - image: images generated by code execution (stored as bytes in images list)
    
    The model can retry failed code up to 5 times, so a single response may
    contain multiple code → code_result cycles. The outcome field tells you
    which succeeded and which failed before a retry.
    """
    trace = []
    text_parts = []
    images = []
    
    for part in response.candidates[0].content.parts:
        if hasattr(part, 'thought') and part.thought:
            trace.append({"type": "thought", "content": part.text or ""})
            continue
        
        if part.text is not None:
            text_parts.append(part.text)
            trace.append({"type": "text", "content": part.text})
        
        if part.executable_code is not None:
            trace.append({"type": "code", "content": part.executable_code.code})
        
        if part.code_execution_result is not None:
            output = part.code_execution_result.output if part.code_execution_result.output else ""
            outcome = str(getattr(part.code_execution_result, 'outcome', 'unknown'))
            trace.append({"type": "code_result", "content": output, "outcome": outcome})
        
        if part.as_image() is not None:
            img_bytes = part.as_image().image_bytes
            images.append(img_bytes)
            # index is temporary — callers replace with filename after saving to disk
            trace.append({"type": "image", "index": len(images) - 1})
    
    full_text = "\n".join(text_parts)
    
    return {
        "text": full_text,
        "images": images,
        "trace": trace
    }


def _save_trace(trace, images, directory, prefix="trace"):
    """Save trace images to disk and update trace entries with filenames.
    
    Mutates trace in place — replaces {"type": "image", "index": N} 
    with {"type": "image", "path": "prefix_N.png"}.
    
    Returns the updated trace (same list, mutated).
    """
    for entry in trace:
        if entry.get("type") == "image" and "index" in entry:
            idx = entry.pop("index")
            if idx < len(images):
                filename = f"{prefix}_{idx}.png"
                with open(Path(directory) / filename, "wb") as f:
                    f.write(images[idx])
                entry["path"] = filename
    return trace

def run_pass1(page_png_path, page_name, discipline):
    """
    Pass 1: Full page comprehension.
    
    Returns dict with:
        regions, sheet_reflection, index, cross_references, page_type, discipline,
        _trace (full Gemini output), _region_images (Gemini's own crops)
    """
    client = _get_client()
    
    # Load prompt
    prompt_path = Path(__file__).parent / "prompts" / "pass1.txt"
    prompt_template = prompt_path.read_text()
    prompt = f"{prompt_template}\n\nPAGE NAME: {page_name}\nDISCIPLINE: {discipline}"
    
    # Load image
    image_bytes = Path(page_png_path).read_bytes()
    
    start = time.perf_counter()
    
    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(parts=[
                types.Part.from_bytes(data=image_bytes, mime_type="image/png"),
                types.Part.from_text(text=prompt),
            ])
        ],
        config=types.GenerateContentConfig(
            # NO response_mime_type — let Gemini output naturally
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )
    
    elapsed_ms = int((time.perf_counter() - start) * 1000)
    
    # Collect everything
    collected = _collect_response(response)
    
    # Parse structured data from text
    parsed = _extract_json_from_text(collected["text"])
    
    # Normalize regions
    regions = parsed.get("regions", [])
    for i, region in enumerate(regions):
        if not isinstance(region, dict):
            continue
        # Ensure bbox is a dict
        bbox = region.get("bbox", {})
        if isinstance(bbox, list) and len(bbox) >= 4:
            bbox = {"x0": bbox[0], "y0": bbox[1], "x1": bbox[2], "y1": bbox[3]}
            region["bbox"] = bbox
        # Ensure required fields
        region.setdefault("id", f"region_{i:03d}")
        region.setdefault("type", "unknown")
        region.setdefault("label", "")
        region.setdefault("confidence", 0.0)
    
    # DON'T match images to regions by order — that's fragile.
    # Gemini may output an overview image first, or skip regions, or 
    # output multiple images. All images go to _crop_candidates.
    # ingest.py decides how to use them (or falls back to PIL crop).
    
    result = {
        "page_type": parsed.get("page_type", "unknown"),
        "discipline": parsed.get("discipline", discipline),
        "regions": regions,
        "sheet_reflection": parsed.get("sheet_reflection", ""),
        "index": parsed.get("index", {}),
        "cross_references": parsed.get("cross_references", []),
        "sheet_info": parsed.get("sheet_info", {}),
        "processing_time_ms": elapsed_ms,
        # _crop_candidates: raw image bytes from Gemini's code execution.
        # NOT matched to regions — ingest.py handles that.
        "_crop_candidates": collected["images"],
        "_trace": collected["trace"],
    }
    
    return result


def run_pass2(crop_path, page_path, region, pass1_context):
    """
    Pass 2: Deep pointer analysis.
    
    Returns dict with:
        content_markdown, materials, dimensions, specs, cross_references,
        coordination_notes, questions_answered, + type-specific fields,
        _trace, _trace_images
    """
    client = _get_client()
    
    # Load prompt and fill context
    prompt_path = Path(__file__).parent / "prompts" / "pass2.txt"
    prompt_template = prompt_path.read_text()
    
    # Build context substitutions
    sheet_info = pass1_context.get("sheet_info", {})
    index = pass1_context.get("index", {})
    
    # Format region's Pass 1 impression
    region_index = region.get("region_index", {})
    region_index_text = ""
    if isinstance(region_index, dict):
        for key, val in region_index.items():
            if val:
                region_index_text += f"{key}: {val}\n"
    if not region_index_text:
        region_index_text = region.get("shows", "No prior analysis")
    
    # Format keynotes
    keynotes = index.get("keynotes", [])
    keynotes_text = "\n".join([f"- {k.get('number','?')}: {k.get('text','')}" for k in keynotes]) if keynotes else "None found"
    
    # Format cross refs
    cross_refs = pass1_context.get("cross_references", [])
    cross_refs_text = ", ".join(str(r) for r in cross_refs) if cross_refs else "None"
    
    prompt = prompt_template.format(
        sheet_number=sheet_info.get("number", pass1_context.get("page_name", "Unknown")),
        sheet_title=sheet_info.get("title", ""),
        discipline=pass1_context.get("discipline", ""),
        region_type=region.get("type", "unknown"),
        region_label=region.get("label", ""),
        detail_number_line=f"Detail Number: {region['detail_number']}" if region.get("detail_number") else "",
        sheet_reflection=pass1_context.get("sheet_reflection", ""),
        region_index_text=region_index_text,
        keynotes_text=keynotes_text,
        cross_refs_text=cross_refs_text,
    )
    
    # Load cropped image
    crop_bytes = Path(crop_path).read_bytes()
    
    start = time.perf_counter()
    
    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(parts=[
                types.Part.from_bytes(data=crop_bytes, mime_type="image/png"),
                types.Part.from_text(text=prompt),
            ])
        ],
        config=types.GenerateContentConfig(
            # NO response_mime_type — natural output
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )
    
    elapsed_ms = int((time.perf_counter() - start) * 1000)
    
    collected = _collect_response(response)
    parsed = _extract_json_from_text(collected["text"])
    
    result = {
        "content_markdown": parsed.get("content_markdown", collected["text"]),
        "materials": parsed.get("materials", []),
        "dimensions": parsed.get("dimensions", []),
        "keynotes_referenced": parsed.get("keynotes_referenced", []),
        "specifications": parsed.get("specifications", []),
        "cross_references": parsed.get("cross_references", []),
        "coordination_notes": parsed.get("coordination_notes", []),
        "questions_answered": parsed.get("questions_answered", []),
        # Type-specific (will be empty dicts/lists if not applicable)
        "assembly": parsed.get("assembly", []),
        "connections": parsed.get("connections", []),
        "areas": parsed.get("areas", []),
        "equipment": parsed.get("equipment", []),
        "modifications": parsed.get("modifications", []),
        "keynotes": parsed.get("keynotes", []),
        "schedule_type": parsed.get("schedule_type", ""),
        "columns": parsed.get("columns", []),
        "rows": parsed.get("rows", []),
        "note_categories": parsed.get("note_categories", []),
        "processing_time_ms": elapsed_ms,
        "_trace": collected["trace"],
        # Raw image bytes — ingest.py pops this, saves to disk, updates trace paths
        "_trace_images": collected["images"],
    }
    
    return result
```

---

### `knowledge_v13.py` — Loads Knowledge Store Into Memory

```python
# knowledge_v13.py — Loads knowledge_store/ into memory

import json
from pathlib import Path

def load_project(project_name=None):
    """
    Load a project from knowledge_store/ into memory.
    If no project_name given, loads the first (or only) project found.
    
    Returns the full project dict that tools_v13.py searches against.
    """
    store = Path("knowledge_store")
    
    if project_name:
        project_dir = store / project_name
    else:
        # Auto-detect: use first project folder found
        projects = [d for d in store.iterdir() if d.is_dir()] if store.exists() else []
        if not projects:
            print("No projects in knowledge_store/. Run: python ingest.py <folder>")
            return None
        project_dir = projects[0]
    
    # Load project metadata
    project_json = project_dir / "project.json"
    if not project_json.exists():
        print(f"No project.json in {project_dir}")
        return None
    
    with open(project_json) as f:
        project = json.load(f)
    
    # Load aggregated index
    index_path = project_dir / "index.json"
    if index_path.exists():
        with open(index_path) as f:
            project["index"] = json.load(f)
    
    # Load all pages
    project["pages"] = {}
    pages_dir = project_dir / "pages"
    
    if pages_dir.exists():
        for page_dir in sorted(pages_dir.iterdir()):
            if not page_dir.is_dir():
                continue
            
            page_name = page_dir.name
            page = {"name": page_name, "path": str(page_dir)}
            
            # Load Pass 1
            pass1_path = page_dir / "pass1.json"
            if pass1_path.exists():
                with open(pass1_path) as f:
                    pass1 = json.load(f)
                page["sheet_reflection"] = pass1.get("sheet_reflection", "")
                page["page_type"] = pass1.get("page_type", "unknown")
                page["discipline"] = pass1.get("discipline", "")
                page["index"] = pass1.get("index", {})
                page["cross_references"] = pass1.get("cross_references", [])
                page["regions"] = pass1.get("regions", [])
            
            # Load pointers (Pass 2)
            page["pointers"] = {}
            pointers_dir = page_dir / "pointers"
            if pointers_dir.exists():
                for pointer_dir in sorted(pointers_dir.iterdir()):
                    if not pointer_dir.is_dir():
                        continue
                    
                    region_id = pointer_dir.name
                    pass2_path = pointer_dir / "pass2.json"
                    
                    if pass2_path.exists():
                        with open(pass2_path) as f:
                            pointer = json.load(f)
                        pointer["crop_path"] = str(pointer_dir / "crop.png")
                        page["pointers"][region_id] = pointer
            
            project["pages"][page_name] = page
    
    page_count = len(project["pages"])
    pointer_count = sum(len(p.get("pointers", {})) for p in project["pages"].values())
    print(f"Loaded: {project['name']} — {page_count} pages, {pointer_count} pointers")
    
    return project
```

---

### `tools_v13.py` — All Tools

```python
# tools_v13.py — Maestro's complete toolset

from knowledge_v13 import load_project

# Load project into memory at import time
project = load_project()


# =========================================================
# KNOWLEDGE TOOLS (read from memory)
# =========================================================

def list_disciplines():
    """List all disciplines in the project."""
    if not project:
        return "No project loaded. Run: python ingest.py <folder>"
    return project.get("disciplines", [])

def list_pages(discipline: str = None):
    """List all pages, optionally filtered by discipline."""
    if not project:
        return "No project loaded."
    pages = []
    for name, page in project["pages"].items():
        if discipline and page.get("discipline", "").lower() != discipline.lower():
            continue
        pages.append({
            "name": name,
            "type": page.get("page_type", "unknown"),
            "discipline": page.get("discipline", ""),
            "region_count": len(page.get("regions", []))
        })
    return pages

def get_sheet_summary(page_name: str):
    """Get the Pass 1 superintendent briefing for a page."""
    if not project:
        return "No project loaded."
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    return page.get("sheet_reflection", "No summary available")

def get_sheet_index(page_name: str):
    """Get the searchable index for a page (keywords, materials, cross-refs)."""
    if not project:
        return "No project loaded."
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    return page.get("index", {})

def list_regions(page_name: str):
    """List all regions/pointers on a page."""
    if not project:
        return "No project loaded."
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    return [
        {
            "id": r.get("id"),
            "type": r.get("type"),
            "label": r.get("label"),
            "detail_number": r.get("detail_number"),
            "has_pass2": r.get("id", "") in page.get("pointers", {})
                         or any(pid.startswith("r_") for pid in page.get("pointers", {}).keys())
        }
        for r in page.get("regions", [])
    ]

def get_region_detail(page_name: str, region_id: str):
    """Get the Pass 2 deep technical brief for a specific region."""
    if not project:
        return "No project loaded."
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    pointer = page.get("pointers", {}).get(region_id)
    if not pointer:
        return f"Region '{region_id}' not found on '{page_name}'"
    return pointer.get("content_markdown", "No detail available")

def search(query: str):
    """Search across all pages and pointers for a keyword or material."""
    if not project:
        return "No project loaded."
    query_lower = query.lower()
    results = []
    
    idx = project.get("index", {})
    
    # Search aggregated materials
    for material, sources in idx.get("materials", {}).items():
        if query_lower in material.lower():
            results.append({"type": "material", "match": material, "found_in": sources})
    
    # Search aggregated keywords
    for keyword, sources in idx.get("keywords", {}).items():
        if query_lower in keyword.lower():
            results.append({"type": "keyword", "match": keyword, "found_in": sources})
    
    # Search sheet reflections
    for name, page in project["pages"].items():
        if query_lower in page.get("sheet_reflection", "").lower():
            results.append({"type": "page", "match": name, "context": "sheet_reflection"})
        
        # Search pointer content
        for pid, pointer in page.get("pointers", {}).items():
            if query_lower in pointer.get("content_markdown", "").lower():
                results.append({"type": "pointer", "match": f"{name}/{pid}", "context": "content_markdown"})
    
    return results if results else f"No results for '{query}'"

def find_cross_references(page_name: str):
    """Find what other sheets reference this page, and what this page references."""
    if not project:
        return "No project loaded."
    idx = project.get("index", {})
    cross_refs = idx.get("cross_refs", {})
    
    refs_to = cross_refs.get(page_name, [])  # Pages that reference this page
    
    page = project["pages"].get(page_name, {})
    refs_from = page.get("cross_references", [])  # Pages this page references
    
    return {
        "references_from_this_page": refs_from,
        "pages_that_reference_this": refs_to
    }

def list_modifications():
    """List all install/demolish/protect items across the entire project."""
    if not project:
        return "No project loaded."
    idx = project.get("index", {})
    return idx.get("modifications", [])

def check_gaps():
    """List broken cross-references and regions without Pass 2 data."""
    if not project:
        return "No project loaded."
    gaps = []
    
    idx = project.get("index", {})
    gaps.extend([{"type": "broken_ref", "detail": ref} for ref in idx.get("broken_refs", [])])
    
    for name, page in project["pages"].items():
        for region in page.get("regions", []):
            rid = region.get("id", "")
            if rid not in page.get("pointers", {}):
                gaps.append({"type": "missing_pass2", "page": name, "region": rid, "label": region.get("label", "")})
    
    return gaps if gaps else "No gaps found"


# =========================================================
# VISION TOOLS (call Gemini on demand — defined in vision.py)
# =========================================================
# These are imported and registered by the engine files.
# Declared here for tool_declarations only.


# =========================================================
# TOOL DECLARATIONS (model-agnostic — engines adapt format)
# =========================================================

tool_definitions = [
    # Knowledge tools
    {"name": "list_disciplines", "description": "List all disciplines in the project", "params": {}},
    {"name": "list_pages", "description": "List all pages, optionally filtered by discipline", "params": {"discipline": {"type": "string", "description": "Filter by discipline name", "required": False}}},
    {"name": "get_sheet_summary", "description": "Get the superintendent briefing for a page", "params": {"page_name": {"type": "string", "required": True}}},
    {"name": "get_sheet_index", "description": "Get the searchable index for a page (keywords, materials, cross-refs)", "params": {"page_name": {"type": "string", "required": True}}},
    {"name": "list_regions", "description": "List all detail regions on a page", "params": {"page_name": {"type": "string", "required": True}}},
    {"name": "get_region_detail", "description": "Get the deep technical brief for a region/pointer", "params": {"page_name": {"type": "string", "required": True}, "region_id": {"type": "string", "required": True}}},
    {"name": "search", "description": "Search all pages and pointers for a keyword, material, or term", "params": {"query": {"type": "string", "required": True}}},
    {"name": "find_cross_references", "description": "Find what sheets reference a page and what it references", "params": {"page_name": {"type": "string", "required": True}}},
    {"name": "list_modifications", "description": "List all install/demolish/protect items across the project", "params": {}},
    {"name": "check_gaps", "description": "Find broken cross-references and regions missing deep analysis", "params": {}},
    
    # Vision tools
    {"name": "see_page", "description": "Look at the full page image yourself to visually inspect it", "params": {"page_name": {"type": "string", "required": True}}},
    {"name": "see_pointer", "description": "Look at a cropped region image to visually inspect a detail", "params": {"page_name": {"type": "string", "required": True}, "region_id": {"type": "string", "required": True}}},
    {"name": "find_missing_pointer", "description": "Use agentic vision to find a missing region on a page. Sends the page image with existing bboxes drawn on it to Gemini.", "params": {"page_name": {"type": "string", "required": True}, "mission": {"type": "string", "description": "What to look for and where", "required": True}}},
    {"name": "double_check_pointer", "description": "Run deep agentic vision on a pointer for maximum detail extraction. Deeper than Pass 2.", "params": {"page_name": {"type": "string", "required": True}, "region_id": {"type": "string", "required": True}, "mission": {"type": "string", "description": "What specifically to verify or extract", "required": True}}},
    
    # Learning tool
    {"name": "learn", "description": "Learn from feedback. Permanently updates experience.", "params": {"learning_mission": {"type": "string", "required": True}}},
]

# Function map — vision tools added by engine at runtime
tool_functions = {
    "list_disciplines": list_disciplines,
    "list_pages": list_pages,
    "get_sheet_summary": get_sheet_summary,
    "get_sheet_index": get_sheet_index,
    "list_regions": list_regions,
    "get_region_detail": get_region_detail,
    "search": search,
    "find_cross_references": find_cross_references,
    "list_modifications": list_modifications,
    "check_gaps": check_gaps,
    # "see_page", "see_pointer", "find_missing_pointer", "double_check_pointer" — added by engine
    # "learn" — added by engine
}
```

---

### `vision.py` — Visual Inspection Tools

```python
# vision.py — Maestro's eyes

import io
from pathlib import Path
from PIL import Image, ImageDraw
from google import genai
from google.genai import types
from dotenv import load_dotenv
import os

load_dotenv()

BRAIN_MODE_MODEL = "gemini-3-flash-preview"

def _get_client():
    return genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

def _collect_response(response):
    """Same collector as gemini_service.py"""
    from gemini_service import _collect_response as collect
    return collect(response)

def _save_trace(trace, images, directory, prefix="trace"):
    """Same saver as gemini_service.py"""
    from gemini_service import _save_trace as save
    return save(trace, images, directory, prefix)


def see_page(page_name: str, project: dict) -> str:
    """Maestro looks at a full page image. Returns Gemini's description."""
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    
    page_png = Path(page["path"]) / "page.png"
    if not page_png.exists():
        return f"No image for '{page_name}'"
    
    client = _get_client()
    image_bytes = page_png.read_bytes()
    
    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(parts=[
                types.Part.from_bytes(data=image_bytes, mime_type="image/png"),
                types.Part.from_text(text="You are Maestro, looking at a construction plan page. Describe what you see — key areas, details, text, dimensions, anything notable. Be thorough but concise."),
            ])
        ],
        config=types.GenerateContentConfig(
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="medium"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )
    
    collected = _collect_response(response)
    
    # Persist trace + images to page directory
    page_dir = Path(page["path"])
    _save_trace(collected["trace"], collected["images"], page_dir, prefix="see_page")
    
    # Save trace JSON
    import json
    with open(page_dir / "see_page_trace.json", "w") as f:
        json.dump(collected["trace"], f, indent=2)
    
    return collected["text"]


def see_pointer(page_name: str, region_id: str, project: dict) -> str:
    """Maestro looks at a cropped region image. Returns Gemini's description."""
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    
    pointer = page.get("pointers", {}).get(region_id)
    if not pointer:
        return f"Region '{region_id}' not found"
    
    crop_path = Path(pointer.get("crop_path", ""))
    if not crop_path.exists():
        return f"No crop image for region '{region_id}'"
    
    client = _get_client()
    crop_bytes = crop_path.read_bytes()
    
    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(parts=[
                types.Part.from_bytes(data=crop_bytes, mime_type="image/png"),
                types.Part.from_text(text="You are Maestro, looking at a cropped detail from a construction plan. Describe everything you see — every dimension, note, material, callout, and connection. Miss nothing."),
            ])
        ],
        config=types.GenerateContentConfig(
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )
    
    collected = _collect_response(response)
    
    # Persist trace + images to pointer directory
    pointer_dir = Path(pointer["crop_path"]).parent
    _save_trace(collected["trace"], collected["images"], pointer_dir, prefix="see_pointer")
    
    import json
    with open(pointer_dir / "see_pointer_trace.json", "w") as f:
        json.dump(collected["trace"], f, indent=2)
    
    return collected["text"]


def find_missing_pointer(page_name: str, mission: str, project: dict) -> str:
    """
    Maestro tells Gemini to find a missing region on a page.
    Sends the page image WITH existing bboxes drawn on it.
    """
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    
    page_png = Path(page["path"]) / "page.png"
    if not page_png.exists():
        return f"No image for '{page_name}'"
    
    # Draw existing bboxes on the image
    img = Image.open(page_png)
    draw = ImageDraw.Draw(img)
    w, h = img.size
    
    for region in page.get("regions", []):
        bbox = region.get("bbox", {})
        x0 = int(bbox.get("x0", 0) / 1000 * w)
        y0 = int(bbox.get("y0", 0) / 1000 * h)
        x1 = int(bbox.get("x1", 0) / 1000 * w)
        y1 = int(bbox.get("y1", 0) / 1000 * h)
        draw.rectangle([x0, y0, x1, y1], outline="red", width=3)
        label = region.get("label", region.get("id", ""))
        draw.text((x0 + 5, y0 + 5), label, fill="red")
    
    # Convert annotated image to bytes
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    annotated_bytes = buf.getvalue()
    
    client = _get_client()
    
    prompt = f"""You are Maestro's agentic vision system. This construction plan page has existing regions marked with RED bounding boxes.

The red boxes show regions already identified. There is something MISSING.

MISSION: {mission}

Use Python code to:
1. Examine the page image
2. Identify the area described in the mission that is NOT covered by existing red boxes
3. Draw a GREEN bounding box around the missing region
4. Output the bounding box coordinates in 0-1000 normalized scale as: BBOX: x0, y0, x1, y1
5. Describe what you found in that region

Show the annotated image with both the existing red boxes and your new green box."""

    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(parts=[
                types.Part.from_bytes(data=annotated_bytes, mime_type="image/png"),
                types.Part.from_text(text=prompt),
            ])
        ],
        config=types.GenerateContentConfig(
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )
    
    collected = _collect_response(response)
    
    # Persist trace + images to page directory
    page_dir = Path(page["path"])
    _save_trace(collected["trace"], collected["images"], page_dir, prefix="find_missing")
    
    import json
    with open(page_dir / "find_missing_trace.json", "w") as f:
        json.dump(collected["trace"], f, indent=2)
    
    return collected["text"]


def double_check_pointer(page_name: str, region_id: str, mission: str, project: dict) -> str:
    """
    Deep agentic vision on a pointer. Unconstrained — Gemini uses code execution
    to zoom, annotate, measure. Deeper than Pass 2.
    """
    page = project["pages"].get(page_name)
    if not page:
        return f"Page '{page_name}' not found"
    
    pointer = page.get("pointers", {}).get(region_id)
    if not pointer:
        return f"Region '{region_id}' not found"
    
    crop_path = Path(pointer.get("crop_path", ""))
    if not crop_path.exists():
        return f"No crop image for region '{region_id}'"
    
    client = _get_client()
    crop_bytes = crop_path.read_bytes()
    
    prompt = f"""You are Maestro's deep inspection system. A superintendent needs you to verify something specific about this construction detail.

MISSION: {mission}

Use Python code to:
1. Open and examine the image closely
2. Zoom into specific areas if needed (crop and re-examine at higher resolution)
3. Annotate the image to highlight what you find — draw arrows, circles, or boxes
4. Read every dimension, note, and callout visible
5. Show your annotated images

Be extremely thorough. If text is too small, zoom in. If something is ambiguous, say so.
Output your findings as a detailed report."""

    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(parts=[
                types.Part.from_bytes(data=crop_bytes, mime_type="image/png"),
                types.Part.from_text(text=prompt),
            ])
        ],
        config=types.GenerateContentConfig(
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )
    
    collected = _collect_response(response)
    
    # Persist trace + images to pointer directory
    pointer_dir = Path(pointer["crop_path"]).parent
    _save_trace(collected["trace"], collected["images"], pointer_dir, prefix="doublecheck")
    
    import json
    with open(pointer_dir / "doublecheck_trace.json", "w") as f:
        json.dump(collected["trace"], f, indent=2)
    
    return collected["text"]
```

---

### `experience_v13.py` — Updated Identity

```python
# experience_v13.py — Maestro's identity for V13

experience = {
    "soul": "You are Maestro, a builder's partner.",
    
    "purpose": "You help construction superintendents answer questions about their plans. You have deep knowledge from analyzed plan sets — sheet summaries, detail breakdowns, materials, dimensions, and cross-references.",
    
    "tools": "Use your knowledge tools to find answers. Start broad (list_disciplines, list_pages), then drill deep (get_sheet_summary, list_regions, get_region_detail). Use search() for cross-cutting queries. If you need to visually verify something, use see_page or see_pointer. For maximum detail, use double_check_pointer. If something is missing from the knowledge, use find_missing_pointer to discover it.",
    
    "tone": "Be concise and direct. Talk like a teammate on the jobsite, not a robot.",
    
    "boundaries": "Answer based on project knowledge and what you can see in the plans. If something isn't in the knowledge or visible, say so honestly. Use check_gaps to see what the plans don't cover.",
    
    "greeting": "Type your questions. Type 'quit' to exit.",
    
    "farewell": "See you on site, boss.",
}
```

---

### Engine files (`maestro_v13_*.py`)

Same structure as V12 engines, but:
- Import from `experience_v13`, `tools_v13`, `knowledge_v13`
- Register vision tools from `vision.py` (passing `project` reference)
- Register `learn` tool
- Convert `tool_definitions` to model-specific format (Gemini/Claude/OpenAI)
- Everything else (chat loop, `process_message`, `build_system_prompt`) stays the same pattern

---

### Data Flow Summary

```
INGEST (Terminal 1):
  python ingest.py "D:\Plans\CFA Love Field"
      │
      ├─ PDF → PNG (PyMuPDF 200 DPI)
      ├─ Pass 1 → Gemini (no forced JSON, full trace captured)
      │   ├─ Regions with bboxes
      │   ├─ Sheet reflection
      │   ├─ Index (keywords, materials, cross-refs)
      │   ├─ Gemini's crop images (part.as_image)
      │   └─ Full trace (code, results, images)
      ├─ Pass 2 → Gemini per region (same approach)
      │   ├─ content_markdown (technical brief)
      │   ├─ Structured fields (materials, dims, specs)
      │   └─ Full trace
      └─ index.json (aggregated, broken refs flagged)

MAESTRO (Terminal 2):
  python maestro_v13_gemini.py
      │
      ├─ Loads knowledge_store/ into memory
      ├─ Superintendent asks question
      ├─ Maestro uses tools:
      │   ├─ Knowledge tools (instant — from memory)
      │   ├─ Vision tools (on demand — Gemini API call)
      │   └─ Learning tool (modifies experience file)
      └─ Answers the question
```

That's the full picture. What needs adjusting?