# gemini_service.py - Pass 1 + Pass 2 Gemini calls with full trace capture

from __future__ import annotations

import json
import logging
import os
import re
import time
from pathlib import Path
from typing import Any

from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()
logger = logging.getLogger(__name__)

BRAIN_MODE_MODEL = "gemini-3-flash-preview"


def _get_client() -> genai.Client:
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY is not set.")
    return genai.Client(api_key=api_key)


def _clean_json_candidate(candidate: str) -> str:
    candidate = re.sub(r",\s*([}\]])", r"\1", candidate)
    return candidate.strip()


def _extract_json_from_text(text: str) -> dict[str, Any]:
    """Parse JSON from natural text output. Tries full text, code blocks, then brace matching."""
    if not text:
        return {}

    stripped = text.strip()
    if not stripped:
        return {}

    # Try direct JSON.
    try:
        parsed = json.loads(stripped)
        if isinstance(parsed, dict):
            return parsed
    except json.JSONDecodeError:
        pass

    # Try fenced blocks.
    for match in re.finditer(r"```(?:json)?\s*\n?(.*?)\n?```", stripped, re.DOTALL | re.IGNORECASE):
        candidate = _clean_json_candidate(match.group(1))
        if not candidate:
            continue
        try:
            parsed = json.loads(candidate)
            if isinstance(parsed, dict):
                return parsed
        except json.JSONDecodeError:
            continue

    # Try first and last brace.
    start = stripped.find("{")
    end = stripped.rfind("}") + 1
    if start != -1 and end > start:
        candidate = _clean_json_candidate(stripped[start:end])
        try:
            parsed = json.loads(candidate)
            if isinstance(parsed, dict):
                return parsed
        except json.JSONDecodeError:
            pass

    # Try balanced-object scanning.
    depth = 0
    start_idx = -1
    for idx, ch in enumerate(stripped):
        if ch == "{":
            if depth == 0:
                start_idx = idx
            depth += 1
        elif ch == "}":
            if depth > 0:
                depth -= 1
                if depth == 0 and start_idx != -1:
                    candidate = _clean_json_candidate(stripped[start_idx : idx + 1])
                    try:
                        parsed = json.loads(candidate)
                        if isinstance(parsed, dict):
                            return parsed
                    except json.JSONDecodeError:
                        pass
                    start_idx = -1

    return {}


def _collect_response(response: Any) -> dict[str, Any]:
    """Collect all response parts, including text, code, code output, and images."""
    trace: list[dict[str, Any]] = []
    text_parts: list[str] = []
    images: list[bytes] = []

    candidates = getattr(response, "candidates", []) or []
    if not candidates:
        return {"text": "", "images": images, "trace": trace}

    parts = getattr(getattr(candidates[0], "content", None), "parts", []) or []

    for part in parts:
        # Thought text (if available)
        thought = getattr(part, "thought", None)
        if thought:
            thought_text = getattr(part, "text", "") or ""
            trace.append({"type": "thought", "content": thought_text})
            continue

        # Regular text
        part_text = getattr(part, "text", None)
        if part_text is not None:
            text_parts.append(part_text)
            trace.append({"type": "text", "content": part_text})

        # Executable code
        executable_code = getattr(part, "executable_code", None)
        if executable_code is not None:
            trace.append({"type": "code", "content": getattr(executable_code, "code", "")})

        # Code execution output
        code_result = getattr(part, "code_execution_result", None)
        if code_result is not None:
            output = getattr(code_result, "output", "") or ""
            outcome = str(getattr(code_result, "outcome", "unknown"))
            trace.append({"type": "code_result", "content": output, "outcome": outcome})

        # Images generated by code execution
        as_image = getattr(part, "as_image", None)
        image_obj = None
        if callable(as_image):
            try:
                image_obj = as_image()
            except Exception:
                image_obj = None
        if image_obj is not None:
            image_bytes = getattr(image_obj, "image_bytes", None)
            if image_bytes:
                images.append(bytes(image_bytes))
                trace.append({"type": "image", "index": len(images) - 1})

    full_text = "\n".join([chunk for chunk in text_parts if chunk is not None])
    return {"text": full_text, "images": images, "trace": trace}


def _save_trace(trace: list[dict[str, Any]], images: list[bytes], directory: str | Path, prefix: str = "trace") -> list[dict[str, Any]]:
    """Save trace images to disk and replace image indexes with local paths."""
    out_dir = Path(directory)
    out_dir.mkdir(parents=True, exist_ok=True)

    for entry in trace:
        if entry.get("type") == "image" and "index" in entry:
            idx = entry.pop("index")
            if isinstance(idx, int) and 0 <= idx < len(images):
                filename = f"{prefix}_{idx}.png"
                with (out_dir / filename).open("wb") as f:
                    f.write(images[idx])
                entry["path"] = filename
            else:
                entry["path"] = None
    return trace


def _normalize_bbox(raw_bbox: Any) -> dict[str, int]:
    if isinstance(raw_bbox, dict):
        x0 = int(round(float(raw_bbox.get("x0", 0))))
        y0 = int(round(float(raw_bbox.get("y0", 0))))
        x1 = int(round(float(raw_bbox.get("x1", 1000))))
        y1 = int(round(float(raw_bbox.get("y1", 1000))))
    elif isinstance(raw_bbox, list) and len(raw_bbox) >= 4:
        x0 = int(round(float(raw_bbox[0])))
        y0 = int(round(float(raw_bbox[1])))
        x1 = int(round(float(raw_bbox[2])))
        y1 = int(round(float(raw_bbox[3])))
    else:
        return {"x0": 0, "y0": 0, "x1": 1000, "y1": 1000}

    x0 = max(0, min(1000, x0))
    y0 = max(0, min(1000, y0))
    x1 = max(0, min(1000, x1))
    y1 = max(0, min(1000, y1))
    if x1 <= x0:
        x1 = min(1000, x0 + 1)
    if y1 <= y0:
        y1 = min(1000, y0 + 1)
    return {"x0": x0, "y0": y0, "x1": x1, "y1": y1}


def run_pass1(page_png_path: str | Path, page_name: str, discipline: str) -> dict[str, Any]:
    """
    Pass 1: full page comprehension.

    Returns:
      page_type, discipline, regions, sheet_reflection, index, cross_references, sheet_info,
      processing_time_ms, _crop_candidates, _trace
    """
    client = _get_client()

    prompt_path = Path(__file__).parent / "prompts" / "pass1.txt"
    prompt_template = prompt_path.read_text(encoding="utf-8")
    prompt = f"{prompt_template}\n\nPAGE NAME: {page_name}\nDISCIPLINE: {discipline}"

    image_bytes = Path(page_png_path).read_bytes()
    start = time.perf_counter()

    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(
                parts=[
                    types.Part.from_bytes(data=image_bytes, mime_type="image/png"),
                    types.Part.from_text(text=prompt),
                ]
            )
        ],
        config=types.GenerateContentConfig(
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )

    elapsed_ms = int((time.perf_counter() - start) * 1000)
    collected = _collect_response(response)
    parsed = _extract_json_from_text(collected["text"])

    raw_regions = parsed.get("regions", [])
    if not isinstance(raw_regions, list):
        raw_regions = []

    regions: list[dict[str, Any]] = []
    for idx, raw_region in enumerate(raw_regions):
        if not isinstance(raw_region, dict):
            continue

        region = dict(raw_region)
        region["bbox"] = _normalize_bbox(region.get("bbox", {}))
        region.setdefault("id", f"region_{idx:03d}")
        region.setdefault("type", "unknown")
        region.setdefault("label", "")
        region.setdefault("confidence", 0.0)
        regions.append(region)

    result: dict[str, Any] = {
        "page_name": page_name,
        "page_type": parsed.get("page_type", "unknown"),
        "discipline": parsed.get("discipline", discipline),
        "regions": regions,
        "sheet_reflection": parsed.get("sheet_reflection", ""),
        "index": parsed.get("index", {}),
        "cross_references": parsed.get("cross_references", []),
        "sheet_info": parsed.get("sheet_info", {}),
        "processing_time_ms": elapsed_ms,
        "_crop_candidates": collected["images"],
        "_trace": collected["trace"],
    }
    return result


def run_pass2(crop_path: str | Path, page_path: str | Path, region: dict[str, Any], pass1_context: dict[str, Any]) -> dict[str, Any]:
    """
    Pass 2: deep pointer analysis.

    Returns:
      content_markdown, materials, dimensions, specifications, cross_references,
      coordination_notes, questions_answered, type-specific fields, processing_time_ms,
      _trace, _trace_images
    """
    del page_path  # Reserved for future context expansion.

    client = _get_client()

    prompt_path = Path(__file__).parent / "prompts" / "pass2.txt"
    prompt_template = prompt_path.read_text(encoding="utf-8")

    sheet_info = pass1_context.get("sheet_info", {})
    if not isinstance(sheet_info, dict):
        sheet_info = {}
    index = pass1_context.get("index", {})
    if not isinstance(index, dict):
        index = {}

    region_index = region.get("region_index", {})
    region_index_lines: list[str] = []
    if isinstance(region_index, dict):
        for key, val in region_index.items():
            if val:
                region_index_lines.append(f"{key}: {val}")
    if not region_index_lines:
        region_index_lines.append(str(region.get("shows", "No prior analysis")))

    keynotes = index.get("keynotes", [])
    if isinstance(keynotes, list) and keynotes:
        keynote_lines = [f"- {k.get('number', '?')}: {k.get('text', '')}" for k in keynotes if isinstance(k, dict)]
        keynotes_text = "\n".join(keynote_lines) if keynote_lines else "None found"
    else:
        keynotes_text = "None found"

    cross_refs = pass1_context.get("cross_references", [])
    if isinstance(cross_refs, list) and cross_refs:
        cross_refs_text = ", ".join([str(item) for item in cross_refs])
    else:
        cross_refs_text = "None"

    prompt = prompt_template.format(
        sheet_number=sheet_info.get("number", pass1_context.get("page_name", "Unknown")),
        sheet_title=sheet_info.get("title", ""),
        discipline=pass1_context.get("discipline", ""),
        region_type=region.get("type", "unknown"),
        region_label=region.get("label", ""),
        detail_number_line=f"Detail Number: {region['detail_number']}" if region.get("detail_number") else "",
        sheet_reflection=pass1_context.get("sheet_reflection", ""),
        region_index_text="\n".join(region_index_lines),
        keynotes_text=keynotes_text,
        cross_refs_text=cross_refs_text,
    )

    crop_bytes = Path(crop_path).read_bytes()
    start = time.perf_counter()

    response = client.models.generate_content(
        model=BRAIN_MODE_MODEL,
        contents=[
            types.Content(
                parts=[
                    types.Part.from_bytes(data=crop_bytes, mime_type="image/png"),
                    types.Part.from_text(text=prompt),
                ]
            )
        ],
        config=types.GenerateContentConfig(
            temperature=0,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            tools=[types.Tool(code_execution=types.ToolCodeExecution)],
        ),
    )

    elapsed_ms = int((time.perf_counter() - start) * 1000)
    collected = _collect_response(response)
    parsed = _extract_json_from_text(collected["text"])

    result: dict[str, Any] = {
        "content_markdown": parsed.get("content_markdown", collected["text"]),
        "materials": parsed.get("materials", []),
        "dimensions": parsed.get("dimensions", []),
        "keynotes_referenced": parsed.get("keynotes_referenced", []),
        "specifications": parsed.get("specifications", []),
        "cross_references": parsed.get("cross_references", []),
        "coordination_notes": parsed.get("coordination_notes", []),
        "questions_answered": parsed.get("questions_answered", []),
        "assembly": parsed.get("assembly", []),
        "connections": parsed.get("connections", []),
        "areas": parsed.get("areas", []),
        "equipment": parsed.get("equipment", []),
        "modifications": parsed.get("modifications", []),
        "keynotes": parsed.get("keynotes", []),
        "schedule_type": parsed.get("schedule_type", ""),
        "columns": parsed.get("columns", []),
        "rows": parsed.get("rows", []),
        "note_categories": parsed.get("note_categories", []),
        "processing_time_ms": elapsed_ms,
        "_trace": collected["trace"],
        "_trace_images": collected["images"],
    }
    return result

